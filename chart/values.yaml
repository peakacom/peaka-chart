nameOverride: ""
fullnameOverride: ""

imagePullSecrets: []

# Configure the URL and port through which the services will be accessed.
# Required for CORS policy execution and to ensure correct URI references in API and JDBC query responses.
accessUrl:

  # Set the common domain through which the services will be accessed.
  domain: "localhost"

  # Set the common URL scheme for accessing the services.
  scheme: http

  # set the port through which the Studio web application will be accessed.
  # use this port for port-forwarding
  port: 8000

  # set the port through which the JDBC service will be accessed.
  dbcPort: 4567

rootUser:
  email: "root@onpremise.com"
  password: "s3cr3t"


# Create a public & private key pair and feed them to here
# generate a private key with `openssl genpkey -out rsakey.pem -algorithm RSA -pkeyopt rsa_keygen_bits:2048`
# generate a public key with `openssl pkey -in rsakey.pem -pubout -out rsapubkey.pem`
jwtRsaPublicKey: ""
jwtRsaPrivateKey: ""

# enter your openAI api key to use built-in chatGPT.
openAIApiKey:

#set default oauth clients to be used for initializing respective connections
connector:
  credentials:
    enabled: true
    provider:
      google:
        clientId:
        clientSecret:
      google_ads:
        clientId:
        clientSecret:
        developerToken:
      hubspot:
        clientId:
        clientSecret:
      mailchimp:
        clientId:
        clientSecret:
      slack:
        clientId:
        clientSecret:
      intercom:
        clientId:
        clientSecret:
      zoho_crm:
        clientId:
        clientSecret:
      linkedin:
        clientId:
        clientSecret:
      facebook:
        clientId:
        clientSecret:
      pipedrive:
        clientId:
        clientSecret:
      dynamics_365:
        clientId:
        clientSecret:
        tenantId:
      microsoft:
        clientId:
        clientSecret:
      quickbooks_online:
        clientId:
        clientSecret:


# TLS settings
tls:
  enabled: false
  cert: ""
  key: ""

# peaka uses a `liquibase` image to reflect database migrations.
liquibase:
  image:
    name: "be-data-migrator"
    tag: "v0.0.33"
    imagePullPolicy: "IfNotPresent"

# peaka uses postgresql to persist some internal data.
# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml
postgresql:
  enabled: true
  volumePermissions:
    enabled: true
  auth:
    username: "code2db"
    password: "code2db"
    database: "code2db"
    postgresPassword: "postgres"
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000
    persistence:
      size: 4Gi
    initdb:
      scriptsConfigMap: peaka-postgresql-initdb-scripts
      user: "postgres"
      password: "postgres"

# peaka requires an NFS server to implement some functionalities
nfsServer:
  enabled: true
  persistentVolume:
    # pv size of the nfs server
    size: 4Gi
    # keep the nfs pvc on helm uninstalls
    volumeDeletionProtection: true
    # path to be used as the nfs share
    mountPath: /nfsshare
  deployment:
    # it is our recommendation that you run nfs server as root, otherwise is not tested
    containerSecurityContext:
      enabled: true
      privileged: true
    containerPort: 2049
  servicePort: 2049

# nfs share to be used across some Peaka pods
nfsShare:
  # keep the nfs pvc on helm uninstalls
  volumeDeletionProtection: true
  # size of the nfs share
  size: 4Gi
  # what happens to a persistent volume when released from its claim (Retain/Delete)
  persistentVolumeReclaimPolicy: Retain
  # path that is exported by the NFS server
  path: "/"

# thrift store
hiveMetastore:
  enabled: true
  image:
    repository: bitsondatadev/hive-metastore
    tag: latest
    pullPolicy: IfNotPresent
  hadoopHeapSize: "10240"
  # db type to be used as metastore. One of mysql or postgres (only mysql tested)
  metastoreType: mysql
  servicePort: 9083
  # by default, hive metastore connects to minio using default minio user. If you want to change this,
  # create a user by entering accessKey, secretKey and policy in minio.users, then change below two values accordingly.
  minioAccessKey: ""
  minioSecretKey: ""

# s3 compatible object storage configuration. By default, Peaka uses MinIO.
# for the full list of values, see https://github.com/minio/minio/tree/master/helm/minio
minio:
  # do not disable as Peaka is not yet tested with other S3 storage options.
  enabled: true
  mode: standalone
  replicas: 1
  persistence:
    size: 4Gi

# database to be used for thrift store.
# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/mariadb-galera/values.yaml
mariadb:
  enabled: true
  image:
    tag: 10.11.4-debian-11-r3
  replicaCount: 1
  db:
    user: peaka
    password: peaka
    name: metastore_db
  rootUser:
    password: peaka
  galera:
    mariabackup:
      password: peaka
  persistence:
    size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/kafka/values.yaml
kafka:
  enabled: true
  nameOverride: kafka
  provisioning:
    numPartitions: 20
    replicationFactor: 1
  extraConfig: |-
    log.retention.hours=12
    max.message.bytes=50000000
    delete.topic.enable=true
    default.replication.factor: 1
    offsets.topic.replication.factor: 1
  volumePermissions:
    enabled: true
  controller:
    replicaCount: 1
    persistence:
      size: 4Gi
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
    interbroker:
      protocol: PLAINTEXT

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/mongodb/values.yaml
mongodb:
  enabled: true
  architecture: standalone
  useStatefulSet: true
  auth:
    enabled: false
  arbiter:
    enabled: false
  persistence:
    size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/redis/values.yaml
redis:
  enabled: true
  auth:
    enabled: false
  architecture: standalone
  master:
    persistence:
      size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml
postgresqlbigtable:
  enabled: true
  volumePermissions:
    enabled: true
  auth:
    username: "code2db"
    password: "code2db"
    database: "code2db"
    postgresPassword: "postgres"
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000
    persistence:
      size: 4Gi
    initdb:
      scriptsConfigMap: peaka-postgresqlbigtable-initdb-scripts
      user: "postgres"
      password: "postgres"

# Default values for cp-kafka-connect.
kafkaConnect:
  enabled: false

  replicaCount: 1

  ## Image Info
  ## ref: https://hub.docker.com/r/confluentinc/cp-kafka/
  image: debezium/connect
  imageTag: 2.5

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  imagePullPolicy: IfNotPresent

  ## Specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  imagePullSecrets:

  servicePort: 8083

  ## Kafka Connect properties
  ## ref: https://docs.confluent.io/current/connect/userguide.html#configuring-workers
  configurationOverrides:
    "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components"
    "key.converter": "io.confluent.connect.avro.AvroConverter"
    "value.converter": "io.confluent.connect.avro.AvroConverter"
    "key.converter.schemas.enable": "false"
    "value.converter.schemas.enable": "false"
    "internal.key.converter": "org.apache.kafka.connect.json.JsonConverter"
    "internal.value.converter": "org.apache.kafka.connect.json.JsonConverter"
    "config.storage.replication.factor": "3"
    "offset.storage.replication.factor": "3"
    "status.storage.replication.factor": "3"

  ## Kafka Connect JVM Heap Option
  heapOptions: "-Xms512M -Xmx512M"

  ## Additional env variables
  ## CUSTOM_SCRIPT_PATH is the path of the custom shell script to be ran mounted in a volume
  customEnv: { }
  # CUSTOM_SCRIPT_PATH: /etc/scripts/create-connectors.sh

  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
  #  cpu: 100m
  #  memory: 128Mi

  ## Custom pod annotations
  podAnnotations: { }

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: { }

  ## Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: [ ]

  ## Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  ## If the Kafka Chart is disabled a URL and port are required to connect
  ## e.g. gnoble-panther-cp-schema-registry:8081
  cp-schema-registry:
    url: ""

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumeMounts:
  # - name: credentials
  #   mountPath: /etc/creds-volume

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumes:
  # - name: credentials
  #   secret:
  #     secretName: creds

  ## Secret with multiple keys to serve the purpose of multiple secrets
  ## Values for all the keys will be base64 encoded when the Secret is created or updated
  ## ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets:
  # username: kafka123
  # password: connect321

  ## These values are used only when "customEnv.CUSTOM_SCRIPT_PATH" is defined.
  ## "livenessProbe" is required only for the edge cases where the custom script to be ran takes too much time
  ## and errors by the ENTRYPOINT are ignored by the container
  ## As an example such a similar script is added to "cp-helm-charts/examples/create-connectors.sh"
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
  # httpGet:
  #   path: /connectors
  #   port: 8083
  # initialDelaySeconds: 30
  # periodSeconds: 5
  # failureThreshold: 10

# vector db for postgresql
pgvector:
  enabled: true
  db:
    name: vectordb
    schema: studio
    user: vectordb
    password: vectordb
  persistence:
    size: 4Gi
  image:
    repository: "ankane/pgvector"
    version: v0.5.1
  options:
    maxConnections: 1000
    sharedBuffers: 1024MB
  port: 5432
  replicaCount: 1
  extraEnvVars: []

# peaka relies on temporal for its workflow executions
temporal:
  enabled: true
  manager:
    resources:
      limits:
        memory: 256Mi
  temporalCluster:
    version: 1.23.0
    numHistoryShards: 1
    ## by default, peaka uses the default installed postgresql (dependency chart) for default store and visibility
    ## store of temporal. Fill below values if you want to use another database.
    ## See https://github.com/alexandrevilain/temporal-operator/tree/main for details.
    persistence:
      defaultStore:
        dbPlugin: ""
        dbUser: ""
        dbHostName: ""
        dbPort: ""
        passwordSecretName: ""
        passwordSecretKey: ""
      visibilityStore:
        dbPlugin: ""
        dbUser: ""
        dbHostName: ""
        dbPort: ""
        passwordSecretName: ""
        passwordSecretKey: ""

trino:
  fullnameOverride: ""
  nameOverride: ""
  workerNameOverride: ""
  coordinatorNameOverride: ""
  additionalConfigProperties:
    #    - retry-policy=QUERY
    - catalog.management=DYNAMIC
  additionalNodeProperties: {}
  additionalLogProperties: {}
  additionalExchangeManagerProperties: {}
  eventListenerProperties: {}
  additionalCatalogs: {}
  tcpRouteEnabled: true
  accessControl: {}
  image:
    name: trino
    pullPolicy: IfNotPresent
    tag: v0.0.512
  initContainers: {}
  auth: {}
    # Set username and password
    # https://trino.io/docs/current/security/password-file.html#file-format
    # passwordAuth: "username:encrypted-password-with-htpasswd"
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
  coordinator:
    config:
      query:
        maxMemoryPerNode: "9GB"
        maxLength: "1000000000"     # a billion
      memory:
        heapHeadroomPerNode: ""
    jvm:
      maxHeapSize: "25G"
      gcMethod:
        type: "UseG1GC"
        g1:
          heapRegionSize: "32M"
    additionalJVMConfig:
      - -Dfile.encoding=UTF-8
      - --add-opens=java.base/java.nio=ALL-UNNAMED
      - -XX:+UnlockDiagnosticVMOptions
      - -XX:G1NumCollectionsKeepPinned=10000000
      - -XX:+EnableDynamicAgentLoading
    additionalExposedPorts: {}
    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi
    livenessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    readinessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalConfigFiles: { }
  worker:
    config:
      memory:
        heapHeadroomPerNode: ""
      query:
        maxMemoryPerNode: "15GB"
        maxLength: "1000000000"     # a billion
    jvm:
      maxHeapSize: "50G"
      gcMethod:
        type: "UseG1GC"
        g1:
          heapRegionSize: "32M"
    additionalJVMConfig:
      - -Dfile.encoding=UTF-8
      - --add-opens=java.base/java.nio=ALL-UNNAMED
      - -XX:+UnlockDiagnosticVMOptions
      - -XX:G1NumCollectionsKeepPinned=10000000
      - -XX:+EnableDynamicAgentLoading
    additionalConfigFiles: {}
    additionalExposedPorts: {}
    resources: {}
    livenessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    readinessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    nodeSelector: {}
    tolerations: []
    affinity: {}
  server:
    exchangeManager:
      name: "filesystem"
      baseDir: "/tmp/trino-local-file-system-exchange-manager"
    autoscaling:
      enabled: false
      targetCPUUtilizationPercentage: 80
      maxReplicas: 3
    log:
      trino:
        level: INFO
    config:
      path: /etc/trino
      query:
        maxMemory: "20GB"
      authenticationType: ""
      https:
        enabled: false
        port: 8443
        keystore:
          path: ""
    workers: 1
    node:
      environment: production
      dataDir: /data/trino
      pluginDir: /usr/lib/trino/plugin
    coordinatorExtraConfig: ""
    workerExtraConfig: ""
  service:
    type: ClusterIP
    port: 8080
  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
    # Annotations to add to the service account
    annotations: {}

aiRest:
  replicaCount: 1
  image:
    name: "be-ai-rest"
    tag: "v0.2.25"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations: {}

authService:
  replicaCount: 1
  image:
    name: "be-auth-service"
    tag: "v0.0.16"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

cloudGateway:
  replicaCount: 1
  image:
    name: "be-cloud-gateway"
    tag: "v0.0.22"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

collabSharedb:
  replicaCount: 1
  image:
    name: "be-collab-sharedb"
    tag: "v0.0.20"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 15

dataCache:
  replicaCount: 1
  image:
    name: "be-data-cache"
    tag: "v0.0.61"
    imagePullPolicy: "IfNotPresent"
  sidecar:
    imageName: "be-common-flow-utilities-service"
    imageTag: "prod"
    imagePullPolicy: "IfNotPresent"
    resources: {}
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5

dataRest:
  replicaCount: 1
  image:
    name: "be-data-rest"
    tag: "v0.0.179"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcher:
  replicaCount: 1
  image:
    name: "be-dispatcher"
    tag: "v0.0.26"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcherAssigner:
  replicaCount: 1
  image:
    name: "be-dispatcher-assigner"
    tag: "v0.0.12"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcherDlq:
  replicaCount: 1
  image:
    name: "be-dispatcher-dlq"
    tag: "v0.0.13"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

emailService:
  replicaCount: 1
  image:
    name: "be-email-service"
    tag: "v0.0.11"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

metadataService:
  replicaCount: 1
  image:
    name: "be-metadata-service"
    tag: "onpremise"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

permissionService:
  replicaCount: 1
  image:
    name: "be-permission-service"
    tag: "v0.0.28"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

runtimeApi:
  replicaCount: 1
  image:
    name: "be-runtime-api"
    tag: "v0.0.11"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

scheduledFlowRunner:
  replicaCount: 1
  image:
    name: "be-scheduled-flow-runner"
    tag: "v0.0.7"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

secretStoreService:
  replicaCount: 1
  secretEncryptionKey: "XXjAe6xLfVWTG5Rf"
  image:
    name: "be-secret-store-service"
    tag: "v0.0.19"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

studioApi:
  replicaCount: 1
  image:
    name: "be-studio-api"
    tag: "v0.0.179"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

tokenService:
  replicaCount: 1
  image:
    name: "be-token-service"
    tag: "v0.0.35"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

webhookResolver:
  replicaCount: 1
  image:
    name: "be-webhook-resolver"
    tag: "v0.0.31"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

workflowHistory:
  replicaCount: 1
  image:
    name: "be-workflow-history"
    tag: "v0.0.14"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

workflowStarter:
  replicaCount: 1
  image:
    name: "be-workflow-starter"
    tag: "v0.0.67"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}

workflowWorkerExpress:
  replicaCount: 1
  image:
    name: "be-workflow-worker-express"
    tag: "v0.0.67"
    imagePullPolicy: "IfNotPresent"
  sidecar:
    imageName: "be-common-flow-utilities-service"
    imageTag: "prod"
    imagePullPolicy: "IfNotPresent"
    resources: {}
    readinessProbe: {}
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

studioWeb:
  replicaCount: 1
  image:
    name: "fe-studio-app"
    tag: "v1.0.81"
    imagePullPolicy: "IfNotPresent"
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe: {}

traefik:
  enabled: true
  logs:
    general:
      level: "DEBUG"
      format: text
    access:
      enabled: true
      format: json
  service:
    type: ClusterIP
  ports:
    # The name of this one can't be changed as it is used for the readiness and
    # liveness probes, but you can adjust its config to your liking
    traefik:
      port: 9000
      expose: true
      # The exposed port for this service
      exposedPort: 9000
      # The port protocol (TCP/UDP)
      protocol: TCP
    web:
      port: 8000
      # hostPort: 8000
      expose: true
      exposedPort: 80
      # The port protocol (TCP/UDP)
      #nodePort: 30080
      protocol: TCP
      tls:
        enabled: false
    websecure:
      port: 8443
      # hostPort: 8443
      expose: true
      #nodePort: 30443
      exposedPort: 443
      # The port protocol (TCP/UDP)
      protocol: TCP
      tls:
        enabled: true
        # this is the name of a TLSOption definition
    dbc:
      port: 4567
      expose: true
      exposedPort: 4567
      #nodePort: 30567
      protocol: TCP
      tls:
        enabled: false
  globalArguments:
    - "--api.insecure=true"
    - "--api.dashboard=true"
    - "--log.level=DEBUG"
  ingressClass:
    enabled: false

  providers:
    kubernetesCRD:
      enabled: true
      allowCrossNamespace: false
      allowExternalNameServices: false
      allowEmptyServices: false
      # ingressClass: traefik-internal
      # labelSelector: environment=production,method=traefik
      namespaces: [ ]
      # - "default"
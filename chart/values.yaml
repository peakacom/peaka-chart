# global parameters
global:
  storageClass: ""

  nodeSelector: {}

  tolerations: []

nameOverride: ""
fullnameOverride: ""

# pull secret to fetch Peaka images
# you will be given a secret json to authenticate to Peaka's container image registry.
imagePullSecret:
  name: peaka-docker-registry
  gcpRegistryAuth:
    # enter the json file contents given below
    # E.g.
    # imagePullSecret:
    #   gcpRegistryAuth
    #     password: |-
    #       {
    #       "file": "given-json-secret"
    #       }
    password: ""

# any additional imagePullSecrets for Peaka images
# E.g.
# additionalImagePullSecrets:
#   - imageRegistrySecretName
additionalImagePullSecrets: []

# Configure the URL and port through which the services will be accessed.
# Required for CORS policy execution and to ensure correct URI references in API and JDBC query responses.
accessUrl:

  # Set the common domain through which the services will be accessed.
  domain: localhost

  # Set the common URL scheme for accessing the services.
  scheme: http

  # set the port through which the Studio web application will be accessed.
  # use this port for port-forwarding
  port: 8000

  # set the port through which the JDBC service will be accessed.
  dbcPort: 4567

# initial registered user to sign in to Peaka studio
rootUser:
  email: root@onpremise.com
  password: s3cr3t

# enter your openAI api key to use built-in chatGPT.
openAIApiKey:

# set default oauth clients to be used for initializing respective connections
connector:
  credentials:
    enabled: true
    provider:
      google:
        clientId:
        clientSecret:
      google_ads:
        clientId:
        clientSecret:
        developerToken:
      hubspot:
        clientId:
        clientSecret:
      mailchimp:
        clientId:
        clientSecret:
      slack:
        clientId:
        clientSecret:
      intercom:
        clientId:
        clientSecret:
      zoho_crm:
        clientId:
        clientSecret:
      linkedin:
        clientId:
        clientSecret:
      facebook:
        clientId:
        clientSecret:
      pipedrive:
        clientId:
        clientSecret:
      dynamics_365:
        clientId:
        clientSecret:
        tenantId:
      microsoft:
        clientId:
        clientSecret:
      quickbooks_online:
        clientId:
        clientSecret:


# TLS settings
tls:
  enabled: false
  cert: ""
  key: ""

# service for database migrations
dataMigrator:
  image:
    name: be-data-migrator
    tag: v0.0.47
    imagePullPolicy: IfNotPresent

# peaka uses postgresql to persist some internal data.
# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml
postgresql:
  enabled: true
  volumePermissions:
    enabled: true
  auth:
    username: code2db
    password: code2db
    database: code2db
    postgresPassword: postgres
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000
    persistence:
      size: 4Gi
    initdb:
      scriptsConfigMap: peaka-postgresql-initdb-scripts
      user: postgres
      password: postgres

# metastore for internal iceberg
hiveMetastore:
  enabled: true
  image:
    repository: bitsondatadev/hive-metastore
    tag: latest
    pullPolicy: IfNotPresent
  hadoopHeapSize: 10240
  # db type to be used as metastore. One of mysql or postgres (only mysql tested)
  metastoreType: mysql
  servicePort: 9083
  # by default, hive metastore connects to minio using default minio user. If you want to change this,
  # create a user by entering accessKey, secretKey and policy in minio.users, then change below two values accordingly.
  minioAccessKey: ""
  minioSecretKey: ""

# s3 compatible object storage configuration. By default, Peaka uses MinIO.
# for the full list of values, see https://github.com/minio/minio/tree/master/helm/minio
minio:
  # do not disable as Peaka is not yet tested with other S3 storage options.
  enabled: true
  mode: standalone
  replicas: 1
  persistence:
    size: 4Gi

# database to be used for thrift store.
# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/mariadb-galera/values.yaml
mariadb:
  enabled: true
  image:
    tag: 10.11.4-debian-11-r3
  replicaCount: 1
  db:
    user: peaka
    password: peaka
    name: metastore_db
  rootUser:
    password: peaka
  galera:
    mariabackup:
      password: peaka
  persistence:
    size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/kafka/values.yaml
kafka:
  enabled: true
  nameOverride: kafka
  provisioning:
    numPartitions: 20
    replicationFactor: 1
  extraConfig: |-
    log.retention.hours=12
    max.message.bytes=50000000
    delete.topic.enable=true
    default.replication.factor: 1
    offsets.topic.replication.factor: 1
  volumePermissions:
    enabled: true
  controller:
    replicaCount: 1
    persistence:
      size: 4Gi
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
    interbroker:
      protocol: PLAINTEXT

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/mongodb/values.yaml
mongodb:
  enabled: true
  architecture: standalone
  useStatefulSet: true
  auth:
    enabled: false
  arbiter:
    enabled: false
  persistence:
    size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/redis/values.yaml
redis:
  enabled: true
  auth:
    enabled: false
  architecture: standalone
  master:
    persistence:
      size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml
postgresqlbigtable:
  enabled: true
  volumePermissions:
    enabled: true
  auth:
    username: code2db
    password: code2db
    database: code2db
    postgresPassword: postgres
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000
    persistence:
      size: 4Gi
    initdb:
      scriptsConfigMap: peaka-postgresqlbigtable-initdb-scripts
      user: postgres
      password: postgres

permify:
  enabled: true
  image:
    tag: "v1.3.6"
  replicaCount: 1
  app:
    account_id: "recdK3q5xuwJdGjrh"
    server:
      rate_limit: 100000
    service:
      circuit_breaker: false
      watch:
        enabled: false
      schema:
        cache:
          number_of_counters: 1000
          max_cost: 32MiB
      permission:
        bulk_limit: 100
        concurrency_limit: 100
        cache:
          number_of_counters: 10000
          max_cost: 2048MiB
    database:
      engine: postgres
      uri_secret: permify-postgresql-uri-secret
      name: permify
      auto_migrate: true
      max_open_connections: 20
      max_idle_connections: 20
      max_connection_lifetime: 86400s
      max_connection_idle_time: 10800s
      garbage_collection:
        enabled: true
        interval: 200h
        window: 200h
        timeout: 5m

# Default values for cp-kafka-connect.
kafkaConnect:
  enabled: false

  replicaCount: 1

  ## Image Info
  ## ref: https://hub.docker.com/r/confluentinc/cp-kafka/
  image: debezium/connect
  imageTag: 2.5

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  imagePullPolicy: IfNotPresent

  ## Specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  imagePullSecrets:

  servicePort: 8083

  ## Kafka Connect properties
  ## ref: https://docs.confluent.io/current/connect/userguide.html#configuring-workers
  configurationOverrides:
    "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components"
    "key.converter": "io.confluent.connect.avro.AvroConverter"
    "value.converter": "io.confluent.connect.avro.AvroConverter"
    "key.converter.schemas.enable": "false"
    "value.converter.schemas.enable": "false"
    "internal.key.converter": "org.apache.kafka.connect.json.JsonConverter"
    "internal.value.converter": "org.apache.kafka.connect.json.JsonConverter"
    "config.storage.replication.factor": "3"
    "offset.storage.replication.factor": "3"
    "status.storage.replication.factor": "3"

  ## Kafka Connect JVM Heap Option
  heapOptions: "-Xms512M -Xmx512M"

  ## Additional env variables
  ## CUSTOM_SCRIPT_PATH is the path of the custom shell script to be ran mounted in a volume
  customEnv: { }
  # CUSTOM_SCRIPT_PATH: /etc/scripts/create-connectors.sh

  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
  #  cpu: 100m
  #  memory: 128Mi

  ## Custom pod annotations
  podAnnotations: { }

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: { }

  ## Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: [ ]

  ## Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  ## If the Kafka Chart is disabled a URL and port are required to connect
  ## e.g. gnoble-panther-cp-schema-registry:8081
  cp-schema-registry:
    url: ""

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumeMounts:
  # - name: credentials
  #   mountPath: /etc/creds-volume

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumes:
  # - name: credentials
  #   secret:
  #     secretName: creds

  ## Secret with multiple keys to serve the purpose of multiple secrets
  ## Values for all the keys will be base64 encoded when the Secret is created or updated
  ## ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets:
  # username: kafka123
  # password: connect321

  ## These values are used only when "customEnv.CUSTOM_SCRIPT_PATH" is defined.
  ## "livenessProbe" is required only for the edge cases where the custom script to be ran takes too much time
  ## and errors by the ENTRYPOINT are ignored by the container
  ## As an example such a similar script is added to "cp-helm-charts/examples/create-connectors.sh"
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
  # httpGet:
  #   path: /connectors
  #   port: 8083
  # initialDelaySeconds: 30
  # periodSeconds: 5
  # failureThreshold: 10

# vector db for peaka ai
pgvector:
  enabled: true
  db:
    name: vectordb
    schema: studio
    user: vectordb
    password: vectordb
  persistence:
    size: 4Gi
    storageClass: ""
  image:
    repository: ankane/pgvector
    version: v0.5.1
  options:
    maxConnections: 1000
    sharedBuffers: 1024MB
  port: 5432
  replicaCount: 1
  extraEnvVars: []

# peaka relies on temporal for its workflow executions
temporal:
  enabled: true
  manager:
    resources:
      limits:
        memory: 256Mi
  temporalCluster:
    version: 1.23.0
    numHistoryShards: 1
    ## by default, peaka uses the default installed postgresql (dependency chart) for default store and visibility
    ## store of temporal. Fill below values if you want to use another database.
    ## See https://github.com/alexandrevilain/temporal-operator/tree/main for details.
    persistence:
      defaultStore:
        dbPlugin: ""
        dbUser: ""
        dbHostName: ""
        dbPort: ""
        passwordSecretName: ""
        passwordSecretKey: ""
      visibilityStore:
        dbPlugin: ""
        dbUser: ""
        dbHostName: ""
        dbPort: ""
        passwordSecretName: ""
        passwordSecretKey: ""
    admintools:
      enabled: false
    ui:
      enabled: false
    log:
      level: debug

trino:
  fullnameOverride: ""
  nameOverride: ""
  workerNameOverride: ""
  coordinatorNameOverride: ""
  additionalConfigProperties:
    #    - retry-policy=QUERY
    - catalog.management=DYNAMIC
  additionalNodeProperties: {}
  additionalLogProperties: {}
  additionalExchangeManagerProperties: {}
  eventListenerProperties: {}
  additionalCatalogs: {}
  tcpRouteEnabled: true
  accessControl: {}
  image:
    name: trino
    pullPolicy: IfNotPresent
    tag: v0.0.586
  initContainers: {}
  auth: {}
    # Set username and password
    # https://trino.io/docs/current/security/password-file.html#file-format
    # passwordAuth: "username:encrypted-password-with-htpasswd"
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
  coordinator:
    config:
      query:
        maxMemoryPerNode: 9GB
        maxLength: "1000000000"     # a billion
      memory:
        heapHeadroomPerNode: ""
    jvm:
      maxHeapSize: 25G
      gcMethod:
        type: UseG1GC
        g1:
          heapRegionSize: 32M
    additionalJVMConfig:
      - -Dfile.encoding=UTF-8
      - --add-opens=java.base/java.nio=ALL-UNNAMED
      - -XX:+UnlockDiagnosticVMOptions
      - -XX:G1NumCollectionsKeepPinned=10000000
      - -XX:+EnableDynamicAgentLoading
    additionalExposedPorts: {}

    # persistence configuration of trino coordinator
    persistence:
      # additional annotations for trino coordinator pvc.
      annotations: {}
      # additional labels for trino coordinator pvc.
      labels: {}
      # storage size of the trino coordinator persistent volume
      size: 1Gi
      # storageClass of the trino coordinator persistent volume
      storageClass: ""

    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi
    livenessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    readinessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalConfigFiles: {}
  worker:
    config:
      memory:
        heapHeadroomPerNode: ""
      query:
        maxMemoryPerNode: 15GB
        maxLength: "1000000000"     # a billion
    jvm:
      maxHeapSize: 50G
      gcMethod:
        type: UseG1GC
        g1:
          heapRegionSize: 32M
    additionalJVMConfig:
      - -Dfile.encoding=UTF-8
      - --add-opens=java.base/java.nio=ALL-UNNAMED
      - -XX:+UnlockDiagnosticVMOptions
      - -XX:G1NumCollectionsKeepPinned=10000000
      - -XX:+EnableDynamicAgentLoading
    additionalConfigFiles: {}
    additionalExposedPorts: {}
    resources: {}
    livenessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    readinessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    nodeSelector: {}
    tolerations: []
    affinity: {}
  server:
    exchangeManager:
      name: filesystem
      baseDir: /tmp/trino-local-file-system-exchange-manager
    autoscaling:
      enabled: false
      targetCPUUtilizationPercentage: 80
      maxReplicas: 3
    log:
      trino:
        level: INFO
    config:
      path: /etc/trino
      query:
        maxMemory: 20GB
      authenticationType: ""
      https:
        enabled: false
        port: 8443
        keystore:
          path: ""
    workers: 1
    node:
      environment: production
      dataDir: /data/trino
      pluginDir: /usr/lib/trino/plugin
    coordinatorExtraConfig: ""
    workerExtraConfig: ""
  service:
    type: ClusterIP
    port: 8080
  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
    # Annotations to add to the service account
    annotations: {}

aiRest:
  replicaCount: 1
  image:
    name: be-ai-rest
    tag: v0.2.50
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  tolerations: {}

authService:
  replicaCount: 1
  image:
    name: be-auth-service
    tag: v0.0.19
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

cloudGateway:
  replicaCount: 1
  image:
    name: be-cloud-gateway
    tag: v0.0.61
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

collabSharedb:
  replicaCount: 1
  image:
    name: be-collab-sharedb
    tag: v0.0.21
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 15

dataCache:
  replicaCount: 1
  image:
    name: be-data-cache
    tag: v0.0.102
    imagePullPolicy: IfNotPresent
  sidecar:
    imageName: be-common-flow-utilities-service
    imageTag: v0.0.19
    imagePullPolicy: IfNotPresent
    resources: {}
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5

dataRest:
  replicaCount: 1
  image:
    name: be-data-rest
    tag: v0.0.261
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcher:
  replicaCount: 1
  image:
    name: be-dispatcher
    tag: v0.0.26
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcherAssigner:
  replicaCount: 1
  image:
    name: be-dispatcher-assigner
    tag: v0.0.12
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcherDlq:
  replicaCount: 1
  image:
    name: be-dispatcher-dlq
    tag: v0.0.13
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

emailService:
  replicaCount: 1
  image:
    name: be-email-service
    tag: v0.0.11
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

metadataService:
  replicaCount: 1
  image:
    name: be-metadata-service
    tag: onpremise
    imagePullPolicy: IfNotPresent
  initContainer:
    image:
      name: be-metadata-connector
      tag: v1.0.1
      imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  persistence:
    annotations: {}
    labels: {}
    size: 2Gi
    storageClass: ""
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

permissionService:
  replicaCount: 1
  image:
    name: be-permission-service
    tag: v0.0.49
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

runtimeApi:
  replicaCount: 1
  image:
    name: be-runtime-api
    tag: v0.0.17
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

scheduledFlowRunner:
  replicaCount: 1
  image:
    name: be-scheduled-flow-runner
    tag: v0.0.7
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

secretStoreService:
  replicaCount: 1
  secretEncryptionKey: "XXjAe6xLfVWTG5Rf"
  image:
    name: be-secret-store-service
    tag: v0.0.19
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

studioApi:
  replicaCount: 1
  image:
    name: be-studio-api
    tag: onpremise
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

tokenService:
  replicaCount: 1
  image:
    name: be-token-service
    tag: v0.0.65
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

webhookResolver:
  replicaCount: 1
  image:
    name: be-webhook-resolver
    tag: v0.0.32
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

workflowHistory:
  replicaCount: 1
  image:
    name: be-workflow-history
    tag: v0.0.22
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

workflowStarter:
  replicaCount: 1
  image:
    name: be-workflow-starter
    tag: v0.0.86
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

workflowWorkerExpress:
  replicaCount: 1
  image:
    name: be-workflow-worker-express
    tag: v0.0.86
    imagePullPolicy: IfNotPresent
  sidecar:
    imageName: be-common-flow-utilities-service
    imageTag: v0.0.19
    imagePullPolicy: IfNotPresent
    resources: {}
    readinessProbe: {}
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

studioWeb:
  replicaCount: 1
  image:
    name: fe-studio-app
    tag: v1.1.29
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe: {}

traefik:
  enabled: true
  logs:
    general:
      level: DEBUG
      format: text
    access:
      enabled: true
      format: json
  service:
    type: ClusterIP
  ports:
    # The name of this one can't be changed as it is used for the readiness and
    # liveness probes, but you can adjust its config to your liking
    traefik:
      port: 9000
      expose: true
      # The exposed port for this service
      exposedPort: 9000
      # The port protocol (TCP/UDP)
      protocol: TCP
    web:
      port: 8000
      # hostPort: 8000
      expose: true
      exposedPort: 80
      # The port protocol (TCP/UDP)
      #nodePort: 30080
      protocol: TCP
      tls:
        enabled: false
    websecure:
      port: 8443
      # hostPort: 8443
      expose: true
      #nodePort: 30443
      exposedPort: 443
      # The port protocol (TCP/UDP)
      protocol: TCP
      tls:
        enabled: true
        # this is the name of a TLSOption definition
    dbc:
      port: 4567
      expose: true
      exposedPort: 4567
      #nodePort: 30567
      protocol: TCP
      tls:
        enabled: false
  globalArguments:
    - "--api.insecure=true"
    - "--api.dashboard=true"
    - "--log.level=DEBUG"
  ingressClass:
    enabled: false

  providers:
    kubernetesCRD:
      enabled: true
      allowCrossNamespace: false
      allowExternalNameServices: false
      allowEmptyServices: false
      # ingressClass: traefik-internal
      # labelSelector: environment=production,method=traefik
      namespaces: []
      # - "default"
# global parameters
global:
  # Default storage class for persistent volumes.
  # Leave empty ("") to use the clusterâ€™s default storage class.
  storageClass: ""

  # Node selector to constrain a pod to run only on specific nodes.
  # Example: {"disktype": "ssd"}
  nodeSelector: {}

  # Tolerations allow pods to be scheduled on nodes with matching taints.
  # Useful for running workloads on dedicated or tainted nodes.
  tolerations: []

# Override the name of the chart. Useful to avoid collisions or enforce naming conventions.
nameOverride: ""

# Override the full name (release name + chart name).
# Takes precedence over nameOverride when set.
fullnameOverride: ""

# Global container image registry.
# Leave empty ("") to use the default registry provided by Peaka.
# Update this field if you want to pull images from another registry
# (e.g., your private registry or Docker Hub).
imageRegistry: ""

# pull secret to fetch Peaka images
# you will be given a secret json to authenticate to pull Peaka images from its private repository.
# If you're going to use your own image registry, leave this empty.
# Example:
# peakaContainerRegistryAccessSecret:
#   name: peaka-docker-registry
#   gcpRegistryAuth:
#     password: |-
#       {
#       "file": "given-json-secret"
#       }
peakaContainerRegistryAccessSecret: {}

# any additional imagePullSecrets for Peaka images
# E.g.
# imagePullSecrets:
#   - imageRegistrySecretName
imagePullSecrets: []

# Configure the URL and port through which the services will be accessed.
# Required for CORS policy execution and to ensure correct URI references in API and JDBC query responses.
accessUrl:

  # Set the common domain through which the services will be accessed.
  domain: localhost

  # Set the common URL scheme for accessing the services.
  scheme: http

  # set the port through which the Studio web application will be accessed.
  # use this port for port-forwarding
  port: 8000

  # set the port through which the JDBC service will be accessed.
  dbcPort: 4567

# initial registered user to sign in to Peaka studio
rootUser:
  email: root@onpremise.com
  password: s3cr3t

# enter your openAI api key to use built-in chatGPT.
openAIApiKey:

# set default oauth clients to be used for initializing respective connections
connector:
  credentials:
    enabled: true
    provider:
      google:
        clientId:
        clientSecret:
      google_ads:
        clientId:
        clientSecret:
        developerToken:
      hubspot:
        clientId:
        clientSecret:
      mailchimp:
        clientId:
        clientSecret:
      slack:
        clientId:
        clientSecret:
      intercom:
        clientId:
        clientSecret:
      zoho_crm:
        clientId:
        clientSecret:
      linkedin:
        clientId:
        clientSecret:
      facebook:
        clientId:
        clientSecret:
      pipedrive:
        clientId:
        clientSecret:
      dynamics_365:
        clientId:
        clientSecret:
        tenantId:
      microsoft:
        clientId:
        clientSecret:
      quickbooks_online:
        clientId:
        clientSecret:


# TLS settings
tls:
  enabled: false
  cert: ""
  key: ""

# service for database migrations
dataMigrator:
  image:
    name: be-data-migrator
    tag: v0.0.51
    imagePullPolicy: IfNotPresent

# peaka uses postgresql to persist some internal data.
# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml
# If externalPostgresql is used, set postgresql.enabled to false
postgresql:
  enabled: true
  image:
    repository: bitnamilegacy/postgresql
  auth:
    username: code2db
    password: code2db
    database: code2db
    postgresPassword: postgres
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000
    persistence:
      size: 4Gi
    initdb:
      scriptsConfigMap: peaka-postgresql-initdb-scripts
      user: postgres
      password: postgres

# @param externalPostgresql Set the configuration below if you want to use some existing postgresql server
# If internal postgresql is used, set externalPostgresql.enabled to false
externalPostgresql:
  enabled: false
  # The IP or DNS of the Postgresql server
  host: ""
  port: 5432
  username: code2db
  password: code2db
  database: code2db
  postgresPassword: postgres

# metastore for internal iceberg
hiveMetastore:
  enabled: true
  image:
    repository: bitsondatadev/hive-metastore
    tag: latest
    pullPolicy: IfNotPresent
  hadoopHeapSize: 10240
  # db type to be used as metastore. One of mysql or postgres (only mysql tested)
  metastoreType: mysql
  servicePort: 9083
  # by default, hive metastore connects to minio using default minio user. If you want to change this,
  # create a user by entering accessKey, secretKey and policy in minio.users, then change below two values accordingly.
  minioAccessKey: ""
  minioSecretKey: ""

# s3 compatible object storage configuration. By default, Peaka uses MinIO.
# for the full list of values, see https://github.com/minio/minio/tree/master/helm/minio
minio:
  # do not disable as Peaka is not yet tested with other S3 storage options.
  enabled: true
  mode: standalone
  replicas: 1
  persistence:
    size: 4Gi

# database to be used for thrift store.
# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/mariadb-galera/values.yaml
mariadb:
  enabled: true
  image:
    repository: bitnamilegacy/mariadb-galera
    tag: 10.11.4-debian-11-r3
  replicaCount: 1
  db:
    user: peaka
    password: peaka
    name: metastore_db
  rootUser:
    password: peaka
  galera:
    mariabackup:
      password: peaka
  persistence:
    size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/kafka/values.yaml
kafka:
  enabled: true
  nameOverride: kafka
  image:
    repository: bitnamilegacy/kafka
  provisioning:
    numPartitions: 20
    replicationFactor: 1
  extraConfig: |-
    log.retention.hours=12
    max.message.bytes=50000000
    delete.topic.enable=true
    default.replication.factor: 1
    offsets.topic.replication.factor: 1
  controller:
    replicaCount: 1
    persistence:
      size: 4Gi
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
    interbroker:
      protocol: PLAINTEXT

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/mongodb/values.yaml
mongodb:
  enabled: true
  image:
    repository: bitnamilegacy/mongodb
  architecture: standalone
  useStatefulSet: true
  auth:
    enabled: false
  arbiter:
    enabled: false
  persistence:
    size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/redis/values.yaml
redis:
  enabled: true
  global:
    security:
      allowInsecureImages: true
  auth:
    enabled: false
  image:
    repository: bitnamilegacy/redis
  architecture: standalone
  master:
    persistence:
      size: 4Gi

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml
postgresqlbigtable:
  enabled: true
  image:
    repository: bitnamilegacy/postgresql
  auth:
    username: code2db
    password: code2db
    database: code2db
    postgresPassword: postgres
  primary:
    extendedConfiguration: |-
      idle_session_timeout = 600000
      max_connections = 1000
    persistence:
      size: 4Gi
    initdb:
      scriptsConfigMap: peaka-postgresqlbigtable-initdb-scripts
      user: postgres
      password: postgres

# for the full list of values, see https://github.com/bitnami/charts/blob/main/bitnami/clickhouse/values.yaml
clickhouse:
  enabled: false
  keeper:
    enabled: false
  replicaCount: 1
  shards: 1
  auth:
    username: peaka
    password: peakaclickhouse123

permify:
  enabled: true
  image:
    tag: "v1.3.6"
  replicaCount: 1
  app:
    account_id: "recdK3q5xuwJdGjrh"
    server:
      rate_limit: 100000
    service:
      circuit_breaker: false
      watch:
        enabled: false
      schema:
        cache:
          number_of_counters: 1000
          max_cost: 32MiB
      permission:
        bulk_limit: 100
        concurrency_limit: 100
        cache:
          number_of_counters: 10000
          max_cost: 2048MiB
    database:
      engine: postgres
      uri_secret: permify-postgresql-uri-secret
      name: permify
      auto_migrate: true
      max_open_connections: 20
      max_idle_connections: 20
      max_connection_lifetime: 86400s
      max_connection_idle_time: 10800s
      garbage_collection:
        enabled: true
        interval: 200h
        window: 200h
        timeout: 5m

pgcat:
  enabled: true
  serviceAccount:
    create: true
    annotations: {}
    ## The name of the service account to use.
    ## If not set and create is true, a name is generated using the fullname template
    name: ""
  service:
    type: ClusterIP
    port: 6432
  image:
    repository: ghcr.io/postgresml/pgcat
    tag: "v1.2.0"
  resources:
    limits: {}
    requests: {}
  configuration:
    general:
      ## @param configuration.general.host What IP to run on, 0.0.0.0 means accessible from everywhere.
      host: "0.0.0.0"

      ## @param configuration.general.port Port to run on, same as PgBouncer used in this example.
      port: 6432

      ## @param configuration.general.enable_prometheus_exporter Whether to enable prometheus exporter or not.
      enable_prometheus_exporter: false

      ## @param configuration.general.prometheus_exporter_port Port at which prometheus exporter listens on.
      prometheus_exporter_port: 9930

      # How long an idle connection with a server is left open (ms).
      idle_timeout: 65000

      # @param configuration.general.connect_timeout How long to wait before aborting a server connection (ms).
      connect_timeout: 5000

      # Max connection lifetime before it's closed, even if actively used.
      server_lifetime: 86400000  # 24 hours

      # Whether to use TLS for server connections or not.
      server_tls: false

      # How long a client is allowed to be idle while in a transaction (ms).
      idle_client_in_transaction_timeout: 0  # milliseconds

      # @param configuration.general.healthcheck_timeout How much time to give `SELECT 1` health check query to return with a result (ms).
      healthcheck_timeout: 1000

      # @param configuration.general.healthcheck_delay How long to keep connection available for immediate re-use, without running a healthcheck query on it
      healthcheck_delay: 30000

      # @param configuration.general.shutdown_timeout How much time to give clients during shutdown before forcibly killing client connections (ms).
      shutdown_timeout: 60000

      # @param configuration.general.ban_time For how long to ban a server if it fails a health check (seconds).
      ban_time: 60    # seconds

      # @param configuration.general.log_client_connections If we should log client connections
      log_client_connections: false

      # @param configuration.general.log_client_disconnections If we should log client disconnections
      log_client_disconnections: false

      # TLS
      # tls_certificate: "server.cert"
      # tls_private_key: "server.key"
      tls_certificate: "-"
      tls_private_key: "-"

      # Credentials to access the virtual administrative database (pgbouncer or pgcat)
      # Connecting to that database allows running commands like `SHOW POOLS`, `SHOW DATABASES`, etc..
      admin_username: "postgres"
      admin_password: "postgres"

      # Query to be sent to servers to obtain the hash used for md5 authentication. The connection will be
      # established using the database configured in the pool. This parameter is inherited by every pool and
      # can be redefined in pool configuration.
      auth_query: null

      # User to be used for connecting to servers to obtain the hash used for md5 authentication by sending
      # the query specified in auth_query_user. The connection will be established using the database configured
      # in the pool. This parameter is inherited by every pool and can be redefined in pool configuration.
      #
      # @param configuration.general.auth_query_user
      auth_query_user: null

      # Password to be used for connecting to servers to obtain the hash used for md5 authentication by sending
      # the query specified in auth_query_user. The connection will be established using the database configured
      # in the pool. This parameter is inherited by every pool and can be redefined in pool configuration.
      #
      # @param configuration.general.auth_query_password
      auth_query_password: null

      # Number of seconds of connection idleness to wait before sending a keepalive packet to the server.
      tcp_keepalives_idle: 5

      # Number of unacknowledged keepalive packets allowed before giving up and closing the connection.
      tcp_keepalives_count: 5

      # Number of seconds between keepalive packets.
      tcp_keepalives_interval: 5
    pools:
      [ {
        name: "permify",
        pool_mode: "session",
        query_parser_enabled: "true",
        query_parser_read_write_splitting: "true",
      } ]

# Default values for cp-kafka-connect.
kafkaConnect:
  enabled: true

  replicaCount: 1

  ## Image Info
  ## ref: https://hub.docker.com/r/confluentinc/cp-kafka/
  image: quay.io/debezium/connect
  imageTag: 3.1

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  imagePullPolicy: IfNotPresent

  ## Specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  imagePullSecrets:

  servicePort: 8083

  ## Kafka Connect properties
  ## ref: https://docs.confluent.io/current/connect/userguide.html#configuring-workers
  configurationOverrides:
    "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components"
    "key.converter": "io.confluent.connect.avro.AvroConverter"
    "value.converter": "io.confluent.connect.avro.AvroConverter"
    "key.converter.schemas.enable": "false"
    "value.converter.schemas.enable": "false"
    "internal.key.converter": "org.apache.kafka.connect.json.JsonConverter"
    "internal.value.converter": "org.apache.kafka.connect.json.JsonConverter"
    "config.storage.replication.factor": "1"
    "offset.storage.replication.factor": "1"
    "status.storage.replication.factor": "1"

  ## Kafka Connect JVM Heap Option
  heapOptions: "-Xms512M -Xmx512M"

  ## Additional env variables
  ## CUSTOM_SCRIPT_PATH is the path of the custom shell script to be ran mounted in a volume
  customEnv: { }
  # CUSTOM_SCRIPT_PATH: /etc/scripts/create-connectors.sh

  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
  #  cpu: 100m
  #  memory: 128Mi

  ## Custom pod annotations
  podAnnotations: { }

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: { }

  ## Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: [ ]

  ## Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  ## If the Kafka Chart is disabled a URL and port are required to connect
  ## e.g. gnoble-panther-cp-schema-registry:8081
  cp-schema-registry:
    url: ""

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumeMounts:
  # - name: credentials
  #   mountPath: /etc/creds-volume

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumes:
  # - name: credentials
  #   secret:
  #     secretName: creds

  ## Secret with multiple keys to serve the purpose of multiple secrets
  ## Values for all the keys will be base64 encoded when the Secret is created or updated
  ## ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets:
  # username: kafka123
  # password: connect321

  ## These values are used only when "customEnv.CUSTOM_SCRIPT_PATH" is defined.
  ## "livenessProbe" is required only for the edge cases where the custom script to be ran takes too much time
  ## and errors by the ENTRYPOINT are ignored by the container
  ## As an example such a similar script is added to "cp-helm-charts/examples/create-connectors.sh"
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
  # httpGet:
  #   path: /connectors
  #   port: 8083
  # initialDelaySeconds: 30
  # periodSeconds: 5
  # failureThreshold: 10

# Default values for monitoring-kafka-connect
monitoringKafkaConnect:
  enabled: true
  # Default values for cp-kafka-connect.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  replicaCount: 1

  ## Image Info
  ## ref: https://hub.docker.com/r/confluentinc/cp-kafka/
  image: code2io/peaka-kafka-connect
  imageTag: v1.0.1

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  imagePullPolicy: Always

  ## Specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  imagePullSecrets:

  servicePort: 8083

  ## Kafka Connect properties
  ## ref: https://docs.confluent.io/current/connect/userguide.html#configuring-workers
  configurationOverrides:
    "plugin.path": "/usr/share/java,/usr/share/confluent-hub-components"
    "key.converter": "org.apache.kafka.connect.json.JsonConverter"
    "value.converter": "org.apache.kafka.connect.json.JsonConverter"
    "internal.key.converter": "org.apache.kafka.connect.json.JsonConverter"
    "internal.value.converter": "org.apache.kafka.connect.json.JsonConverter"
    "config.storage.replication.factor": "1"
    "offset.storage.replication.factor": "1"
    "status.storage.replication.factor": "1"

  ## Kafka Connect JVM Heap Option
  heapOptions: "-Xms512M -Xmx4096M"

  ## Additional env variables
  ## CUSTOM_SCRIPT_PATH is the path of the custom shell script to be ran mounted in a volume
  customEnv: { }
  # CUSTOM_SCRIPT_PATH: /etc/scripts/create-connectors.sh

  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
  #  cpu: 100m
  #  memory: 128Mi

  ## Custom pod annotations
  podAnnotations: { }

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: { }

  ## Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: [ ]

  ## Pod scheduling constraints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: { }

  ## Monitoring
  ## Kafka Connect JMX Settings
  ## ref: https://kafka.apache.org/documentation/#connect_monitoring
  jmx:
    port: 5555

  ## Prometheus Exporter Configuration
  ## ref: https://prometheus.io/docs/instrumenting/exporters/
  prometheus:
    ## JMX Exporter Configuration
    ## ref: https://github.com/prometheus/jmx_exporter
    jmx:
      enabled: false
      image: solsson/kafka-prometheus-jmx-exporter@sha256
      imageTag: 6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143
      imagePullPolicy: IfNotPresent
      port: 5556

      ## Resources configuration for the JMX exporter container.
      ## See the `resources` documentation above for details.
      resources: { }

  ## You can list load balanced service endpoint, or list of all brokers (which is hard in K8s).  e.g.:
  ## bootstrapServers: "PLAINTEXT://dozing-prawn-kafka-headless:9092"
  kafka:
    bootstrapServers: ""

  ## If the Kafka Chart is disabled a URL and port are required to connect
  ## e.g. gnoble-panther-cp-schema-registry:8081
  cp-schema-registry:
    url: ""

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumeMounts: []

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  volumes: []

  ## Secret with multiple keys to serve the purpose of multiple secrets
  ## Values for all the keys will be base64 encoded when the Secret is created or updated
  ## ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets:
  # username: kafka123
  # password: connect321

  ## These values are used only when "customEnv.CUSTOM_SCRIPT_PATH" is defined.
  ## "livenessProbe" is required only for the edge cases where the custom script to be ran takes too much time
  ## and errors by the ENTRYPOINT are ignored by the container
  ## As an example such a similar script is added to "cp-helm-charts/examples/create-connectors.sh"
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
  # httpGet:
  #   path: /connectors
  #   port: 8083
  # initialDelaySeconds: 30
  # periodSeconds: 5
  # failureThreshold: 10


# vector db for peaka ai
pgvector:
  enabled: true
  db:
    name: vectordb
    schema: studio
    user: vectordb
    password: vectordb
  persistence:
    size: 4Gi
    storageClass: ""
  image:
    repository: ankane/pgvector
    version: v0.5.1
  options:
    maxConnections: 1000
    sharedBuffers: 1024MB
  port: 5432
  replicaCount: 1
  extraEnvVars: []

# peaka relies on temporal for its workflow executions
temporal:
  enabled: true
  nameOverride: ""
  fullnameOverride: ""

  debug: false

  # Custom Service account management
  serviceAccount:
    # Whether to create service account or not
    create: false

    # Name of the service account, default: temporal.fullname
    name:

    # extraAnnotations would let users add additional annotations
    extraAnnotations:

  server:
    enabled: true
    sidecarContainers: {}
    image:
      repository: temporalio/server
      tag: 1.22.4
      pullPolicy: IfNotPresent

    # Global default settings (can be overridden per service)
    replicaCount: 1
    metrics:
      # Annotate pods directly with Prometheus annotations.
      # Use this if you installed Prometheus from a Helm chart.
      annotations:
        enabled: true
      # Additional tags to be added to Prometheus metrics
      tags: {}
      # Enable Prometheus ServiceMonitor
      # Use this if you installed the Prometheus Operator (https://github.com/coreos/prometheus-operator).
      serviceMonitor:
        enabled: false
        interval: 30s
        # Set additional lables to all the ServiceMonitor resources
        additionalLabels: {}
        #  label1: value1
        #  label2: value2
        # Set Prometheus metric_relabel_configs via ServiceMonitor
        # Use metricRelabelings to adjust metric and label names as needed
        metricRelabelings: []
        # - action: replace
        #   sourceLabels:
        #   - exported_namespace
        #   targetLabel: temporal_namespace
        # - action: replace
        #   regex: service_errors_(.+)
        #   replacement: ${1}
        #   sourceLabels:
        #   - __name__
        #   targetLabel: temporal_error_kind
        # - action: replace
        #   regex: service_errors_.+
        #   replacement: temporal_service_errors
        #   sourceLabels:
        #   - __name__
        #   targetLabel: __name__
      prometheus:
        timerType: histogram
    podAnnotations: {}
    podLabels: {}
    secretLabels: {}
    secretAnnotations: {}
    resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
    #  cpu: 100m
    #  memory: 128Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalVolumes: []
    additionalVolumeMounts: []
    additionalEnv: []
    securityContext:
      fsGroup: 1000
      runAsUser: 1000

    config:
      logLevel: "debug,info"

      # IMPORTANT: This value cannot be changed, once it's set.
      numHistoryShards: 512

      # Define your TLS configuration here. See https://docs.temporal.io/references/configuration#tls
      # for configuration options. You must also use `server.additionalVolumeMounts` and `server.additionalVolumes`
      # to mount certificates (from Secret or ConfigMap etc) to the path you use below.
      # tls:
      #   internode:
      #     server:
      #       certFile: /path/to/internode/cert/file
      #       keyFile: /path/to/internode/key/file
      #       requireClientAuth: true
      #       clientCaFiles:
      #         - /path/to/internode/serverCa
      #     client:
      #       serverName: dnsSanInInternodeCertificate
      #       rootCaFiles:
      #         - /path/to/internode/serverCa
      #   frontend:
      #     server:
      #       certFile: /path/to/frontend/cert/file
      #       keyFile: /path/to/frontend/key/file
      #       requireClientAuth: true
      #       clientCaFiles:
      #         - /path/to/internode/serverCa
      #         - /path/to/sdkClientPool1/ca
      #         - /path/to/sdkClientPool2/ca
      #     client:
      #       serverName: dnsSanInFrontendCertificate
      #       rootCaFiles:
      #         - /path/to/frontend/serverCa

      persistence:
        defaultStore: default
        additionalStores: {}

        default:
          driver: "sql"

          cassandra:
            hosts: []
            # port: 9042
            keyspace: "temporal"
            user: "user"
            password: "password"
            existingSecret: ""
            replicationFactor: 1
            consistency:
              default:
                consistency: "local_quorum"
                serialConsistency: "local_serial"
            # datacenter: "us-east-1a"
            # maxQPS: 1000
            # maxConns: 2

          sql:
            driver: ""
            host: ""
            port: 5432
            database: ""
            user: ""
            password: ""
            existingSecret: ""
            secretName: ""
            maxConns: 20
            maxConnLifetime: "1h"
            # connectAttributes:
            # tx_isolation: 'READ-COMMITTED'

        visibility:
          driver: "sql"

          cassandra:
            hosts: []
            # port: 9042
            keyspace: "temporal_visibility"
            user: "user"
            password: "password"
            existingSecret: ""
            # datacenter: "us-east-1a"
            # maxQPS: 1000
            # maxConns: 2
            replicationFactor: 1
            consistency:
              default:
                consistency: "local_quorum"
                serialConsistency: "local_serial"

          sql:
            driver: ""
            host: ""
            port: 5432
            database: ""
            user: ""
            password: ""
            existingSecret: ""
            secretName: ""
            maxConns: 20
            maxConnLifetime: "1h"
            # connectAttributes:
            #   tx_isolation: 'READ-COMMITTED'

    frontend:
      service:
        annotations: {} # Evaluated as template
        type: ClusterIP
        port: 7233
      metrics:
        annotations:
          enabled: true
        serviceMonitor: {}
        # enabled: false
        prometheus: {}
        # timerType: histogram
      podAnnotations: {}
      podLabels: {}
      resources: {}
      nodeSelector: {}
      tolerations: []
      affinity: {}
      additionalEnv: []
      containerSecurityContext: {}
      topologySpreadConstraints: {}
      podDisruptionBudget: {}

    history:
      service:
        # type: ClusterIP
        port: 7234
      metrics:
        annotations:
          enabled: true
        serviceMonitor: {}
        # enabled: false
        prometheus: {}
        # timerType: histogram
      podAnnotations: {}
      podLabels: {}
      resources: {}
      nodeSelector: {}
      tolerations: []
      affinity: {}
      additionalEnv: []
      containerSecurityContext: {}
      topologySpreadConstraints: {}
      podDisruptionBudget: {}

    matching:
      service:
        # type: ClusterIP
        port: 7235
      metrics:
        annotations:
          enabled: false
        serviceMonitor: {}
        # enabled: false
        prometheus: {}
        # timerType: histogram
      podAnnotations: {}
      podLabels: {}
      resources: {}
      nodeSelector: {}
      tolerations: []
      affinity: {}
      additionalEnv: []
      containerSecurityContext: {}
      topologySpreadConstraints: {}
      podDisruptionBudget: {}

    worker:
      service:
        # type: ClusterIP
        port: 7239
      metrics:
        annotations:
          enabled: true
        serviceMonitor: {}
        # enabled: false
        prometheus: {}
        # timerType: histogram
      podAnnotations: {}
      podLabels: {}
      resources: {}
      nodeSelector: {}
      tolerations: []
      affinity: {}
      additionalEnv: []
      containerSecurityContext: {}
      topologySpreadConstraints: {}
      podDisruptionBudget: {}

  postgresql:
    enabled: true

  admintools:
    enabled: false
    image:
      repository: temporalio/admin-tools
      tag: 1.22.4
      pullPolicy: IfNotPresent

    service:
      type: ClusterIP
      port: 22
      annotations: {}
    podLabels: {}
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalEnv: []
    resources: {}
    containerSecurityContext: {}
    securityContext: {}
    podDisruptionBudget: {}

  web:
    enabled: false
    config:
      # server/config.yml file content
      auth:
        enabled: false
      routing:
        default_to_namespace: # internal use only
        issue_report_link: https://github.com/temporalio/web/issues/new/choose # set this field if you need to direct people to internal support forums

    replicaCount: 1

    image:
      repository: temporalio/ui
      tag: 2.16.2
      pullPolicy: IfNotPresent

    service:
      # set type to NodePort if access to web needs access from outside the cluster
      # for more info see https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
      type: ClusterIP
      port: 8080
      annotations: {}
      # loadBalancerIP:

    podAnnotations: {}
    podLabels: {}

    resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
    #  cpu: 100m
    #  memory: 128Mi

    nodeSelector: {}

    tolerations: []

    affinity: {}

    additionalVolumes: []
    additionalVolumeMounts: []

    additionalEnv: []

    containerSecurityContext: {}

    securityContext: {}

  schema:
    setup:
      enabled: true
      backoffLimit: 100
    update:
      enabled: true
      backoffLimit: 100
    resources: {}
    containerSecurityContext: {}
    securityContext: {}

  elasticsearch:
    enabled: false
    replicas: 3
    persistence:
      enabled: false
    imageTag: 7.17.3
    host: elasticsearch-master-headless
    scheme: http
    port: 9200
    version: "v7"
    logLevel: "error"
    username: ""
    password: ""
    visibilityIndex: "temporal_visibility_v1_dev"

  prometheus:
    enabled: false
    nodeExporter:
      enabled: false

  grafana:
    enabled: false
    replicas: 1
    testFramework:
      enabled: false
    rbac:
      create: false
      pspEnabled: false
      namespaced: true
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: "default"
            orgId: 1
            folder: ""
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: TemporalMetrics
            type: prometheus
            url: http://{{ .Release.Name }}-prometheus-server
            access: proxy
            isDefault: true
    dashboards:
      default:
        server-general-github:
          url: https://raw.githubusercontent.com/temporalio/dashboards/helm/server/server-general.json
          datasource: TemporalMetrics
        sdk-general-github:
          url: https://raw.githubusercontent.com/temporalio/dashboards/helm/sdk/sdk-general.json
          datasource: TemporalMetrics
        misc-advanced-visibility-specific-github:
          url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/advanced-visibility-specific.json
          datasource: TemporalMetrics
        misc-clustermonitoring-kubernetes-github:
          url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/clustermonitoring-kubernetes.json
          datasource: TemporalMetrics
        misc-frontend-service-specific-github:
          url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/frontend-service-specific.json
          datasource: TemporalMetrics
        misc-history-service-specific-github:
          url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/history-service-specific.json
          datasource: TemporalMetrics
        misc-matching-service-specific-github:
          url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/matching-service-specific.json
          datasource: TemporalMetrics
        misc-worker-service-specific-github:
          url: https://raw.githubusercontent.com/temporalio/dashboards/helm/misc/worker-service-specific.json
          datasource: TemporalMetrics

  cassandra:
    enabled: false
    persistence:
      enabled: false
    image:
      repo: cassandra
      tag: 3.11.3
      pullPolicy: IfNotPresent
    config:
      cluster_size: 3
      ports:
        cql: 9042
      num_tokens: 4
      max_heap_size: 512M
      heap_new_size: 128M
      seed_size: 0
    service:
      type: ClusterIP

  mysql:
    enabled: false


trino:
  fullnameOverride: ""
  nameOverride: ""
  workerNameOverride: ""
  coordinatorNameOverride: ""
  additionalConfigProperties:
    #    - retry-policy=QUERY
    - catalog.management=DYNAMIC
  additionalNodeProperties: {}
  additionalLogProperties: {}
  additionalExchangeManagerProperties: {}
  eventListenerProperties: {}
  additionalCatalogs: {}
  tcpRouteEnabled: true
  accessControl: {}
  image:
    name: trino
    pullPolicy: IfNotPresent
    tag: v0.0.614
  initContainers: {}
  auth: {}
    # Set username and password
    # https://trino.io/docs/current/security/password-file.html#file-format
    # passwordAuth: "username:encrypted-password-with-htpasswd"
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
  coordinator:
    config:
      query:
        maxMemoryPerNode: 9GB
        maxLength: "1000000000"     # a billion
      memory:
        heapHeadroomPerNode: ""
    jvm:
      maxHeapSize: 25G
      gcMethod:
        type: UseG1GC
        g1:
          heapRegionSize: 32M
    additionalJVMConfig:
      - -Dfile.encoding=UTF-8
      - --add-opens=java.base/java.nio=ALL-UNNAMED
      - -XX:+UnlockDiagnosticVMOptions
      - -XX:G1NumCollectionsKeepPinned=10000000
      - -XX:+EnableDynamicAgentLoading
    additionalExposedPorts: {}

    # persistence configuration of trino coordinator
    persistence:
      # additional annotations for trino coordinator pvc.
      annotations: {}
      # additional labels for trino coordinator pvc.
      labels: {}
      # storage size of the trino coordinator persistent volume
      size: 1Gi
      # storageClass of the trino coordinator persistent volume
      storageClass: ""

    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi
    livenessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    readinessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    nodeSelector: {}
    tolerations: []
    affinity: {}
    additionalConfigFiles: {}
  worker:
    config:
      memory:
        heapHeadroomPerNode: ""
      query:
        maxMemoryPerNode: 15GB
        maxLength: "1000000000"     # a billion
    jvm:
      maxHeapSize: 50G
      gcMethod:
        type: UseG1GC
        g1:
          heapRegionSize: 32M
    additionalJVMConfig:
      - -Dfile.encoding=UTF-8
      - --add-opens=java.base/java.nio=ALL-UNNAMED
      - -XX:+UnlockDiagnosticVMOptions
      - -XX:G1NumCollectionsKeepPinned=10000000
      - -XX:+EnableDynamicAgentLoading
    additionalConfigFiles: {}
    additionalExposedPorts: {}
    resources: {}
    livenessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    readinessProbe: {}
      # initialDelaySeconds: 20
      # periodSeconds: 10
      # timeoutSeconds: 5
      # failureThreshold: 6
      # successThreshold: 1
    nodeSelector: {}
    tolerations: []
    affinity: {}
  server:
    exchangeManager:
      name: filesystem
      baseDir: /tmp/trino-local-file-system-exchange-manager
    autoscaling:
      enabled: false
      targetCPUUtilizationPercentage: 80
      maxReplicas: 3
    log:
      trino:
        level: INFO
    config:
      path: /etc/trino
      query:
        maxMemory: 20GB
      authenticationType: ""
      https:
        enabled: false
        port: 8443
        keystore:
          path: ""
    workers: 1
    node:
      environment: production
      dataDir: /data/trino
      pluginDir: /usr/lib/trino/plugin
    coordinatorExtraConfig: ""
    workerExtraConfig: ""
  service:
    type: ClusterIP
    port: 8080
  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
    # Annotations to add to the service account
    annotations: {}

authService:
  replicaCount: 1
  image:
    name: be-auth-service
    tag: v0.0.20
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

cloudGateway:
  replicaCount: 1
  image:
    name: be-cloud-gateway
    tag: v0.0.77
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

collabSharedb:
  replicaCount: 1
  image:
    name: be-collab-sharedb
    tag: v0.0.21
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 30
    periodSeconds: 15
    successThreshold: 1
    timeoutSeconds: 15

dataCache:
  replicaCount: 1
  image:
    name: be-data-cache
    tag: v0.0.133
    imagePullPolicy: IfNotPresent
  sidecar:
    imageName: be-common-flow-utilities-service
    imageTag: v0.0.21
    imagePullPolicy: IfNotPresent
    resources: {}
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5

dataRest:
  replicaCount: 1
  image:
    name: be-data-rest
    tag: v0.0.288
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcher:
  replicaCount: 1
  image:
    name: be-dispatcher
    tag: v0.0.27
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcherAssigner:
  replicaCount: 1
  image:
    name: be-dispatcher-assigner
    tag: v0.0.13
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

dispatcherDlq:
  replicaCount: 1
  image:
    name: be-dispatcher-dlq
    tag: v0.0.14
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

emailService:
  replicaCount: 1
  image:
    name: be-email-service
    tag: v0.0.12
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

metadataService:
  replicaCount: 1
  image:
    name: be-metadata-service
    tag: v0.0.26
    imagePullPolicy: IfNotPresent
  initContainer:
    image:
      name: be-metadata-connector
      tag: v1.0.1
      imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  persistence:
    annotations: {}
    labels: {}
    size: 2Gi
    storageClass: ""
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

monitoringService:
  replicaCount: 1
  image:
    name: be-monitoring-service
    tag: v0.0.44
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

permissionService:
  replicaCount: 1
  image:
    name: be-permission-service
    tag: v0.0.57
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

runtimeApi:
  replicaCount: 1
  image:
    name: be-runtime-api
    tag: v0.0.20
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

scheduledFlowRunner:
  replicaCount: 1
  image:
    name: be-scheduled-flow-runner
    tag: v0.0.11
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

searchService:
  replicaCount: 1
  image:
    name: be-search-service
    tag: v0.1.102
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

secretStoreService:
  replicaCount: 1
  secretEncryptionKey: "XXjAe6xLfVWTG5Rf"
  image:
    name: be-secret-store-service
    tag: v0.0.20
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

studioApi:
  replicaCount: 1
  image:
    name: be-studio-api
    tag: v0.0.310
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

tokenService:
  replicaCount: 1
  image:
    name: be-token-service
    tag: v0.0.72
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

webhookResolver:
  replicaCount: 1
  image:
    name: be-webhook-resolver
    tag: v0.0.33
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

workflowHistory:
  replicaCount: 1
  image:
    name: be-workflow-history
    tag: v0.0.23
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

workflowStarter:
  replicaCount: 1
  image:
    name: be-workflow-starter
    tag: v0.0.94
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}

workflowWorkerExpress:
  replicaCount: 1
  image:
    name: be-workflow-worker-express
    tag: v0.0.94
    imagePullPolicy: IfNotPresent
  sidecar:
    imageName: be-common-flow-utilities-service
    imageTag: v0.0.21
    imagePullPolicy: IfNotPresent
    resources: {}
    readinessProbe: {}
  resources: {}
  nodeSelector: {}
  affinity: {}
  livenessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 20
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 3
    httpGet:
      path: /actuator/health
      port: 9090
      scheme: HTTP
    initialDelaySeconds: 60
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5

sqlService:
  replicaCount: 1
  image:
    name: be-sql-service
    tag: v0.0.2
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe: {}

studioWeb:
  replicaCount: 1
  image:
    name: fe-studio-app
    tag: v1.1.92
    imagePullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  affinity: {}
  readinessProbe: {}


# -----------------------------------------------------------------------------
# Service Exposure Settings
# -----------------------------------------------------------------------------
# The Peaka application is accessible through this Traefik service.
# To expose Peaka outside the cluster, configure the service type below.
#
# Options:
#   - ClusterIP    : Internal access only (default)
#   - NodePort     : Expose on each nodeâ€™s IP and a static port
#   - LoadBalancer : Provision an external load balancer (if supported by your cloud)
#
# Ports:
#   - web:
#       Use this if TLS termination happens BEFORE traffic reaches Peaka.
#       Set web.expose = true.
#
#   - websecure:
#       Use this if you want Peaka to terminate TLS itself.
#       Set websecure.expose = true.
#
#   - dbc:
#       Use this if you want to expose the Peaka JDBC service.
#       Set dbc.expose = true.
# IMPORTANT (NodePort / LoadBalancer):
#   If you set service.type = NodePort or LoadBalancer, you should also
#   configure web.nodePort, websecure.nodePort, and/or dbc.nodePort.
#   Otherwise, Kubernetes will assign random node ports.
#
# Example:
# traefik:
#   service:
#     type: LoadBalancer
#   ports:
#     web:
#       expose: true
#       nodePort: 30080
#     websecure:
#       expose: false
#     dbc:
#       expose: true
#       nodePort: 30567
traefik:
  enabled: true
  logs:
    general:
      level: DEBUG
      format: text
    access:
      enabled: true
      format: json
  service:
    type: ClusterIP
  ports:
    # The name of this one can't be changed as it is used for the readiness and
    # liveness probes, but you can adjust its config to your liking
    traefik:
      port: 9000
      expose: false
      # The exposed port for this service
      exposedPort: 9000
      # The port protocol (TCP/UDP)
      protocol: TCP
    web:
      port: 8000
      # hostPort: 8000
      expose: false
      exposedPort: 80
      # The port protocol (TCP/UDP)
      #nodePort: 30080
      protocol: TCP
      tls:
        enabled: false
    websecure:
      port: 8443
      # hostPort: 8443
      expose: false
      #nodePort: 30443
      exposedPort: 443
      # The port protocol (TCP/UDP)
      protocol: TCP
      tls:
        enabled: true
        # this is the name of a TLSOption definition
    dbc:
      port: 4567
      expose: false
      exposedPort: 4567
      #nodePort: 30567
      protocol: TCP
      tls:
        enabled: false
  globalArguments:
    - "--api.insecure=true"
    - "--api.dashboard=true"
    - "--log.level=DEBUG"
  ingressClass:
    enabled: false

  providers:
    kubernetesCRD:
      enabled: true
      allowCrossNamespace: false
      allowExternalNameServices: false
      allowEmptyServices: false
      # ingressClass: traefik-internal
      # labelSelector: environment=production,method=traefik
      namespaces: []
      # - "default"
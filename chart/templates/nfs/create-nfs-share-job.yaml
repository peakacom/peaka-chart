apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "peaka.fullname" . }}-create-nfs-share
spec:
  template:
    spec:
      serviceAccountName: {{ include "peaka.fullname" . }}-nfs-share
      containers:
        - name: create-nfs-share
          image: bash:alpine3.20
          command:
            - /usr/local/bin/bash
            - -c
            - |
              apk add curl
              curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
              chmod +x ./kubectl
              mv ./kubectl /usr/local/bin
              kubectl version

              until nslookup {{ include "peaka.fullname" . }}-nfs-server.{{ .Release.Namespace }}.svc.cluster.local ; do
                echo waiting for nfs-server; sleep 5 ;
              done
              CLUSTER_IP=$(kubectl get svc {{ include "peaka.fullname" . }}-nfs-server -o jsonpath='{.spec.clusterIP}')
              echo "IP of the nfs-server is ${CLUSTER_IP}"

              if [[ $(kubectl get pv {{ include "peaka.fullname" . }}-nfs-share) ]] ; then     # if nfs share already exists

                # nfs share exists and IP of nfs share is the same as IP of nfs-server, this is desired, exit peacefully.
                if [[ $(kubectl get pv {{ include "peaka.fullname" . }}-nfs-share -o jsonpath='{.spec.nfs.server}') == "${CLUSTER_IP}" ]] ; then
                  echo "Current IP of nfs-server and the IP of nfs-share matches. Nothing to do. Exiting."
                  exit 0

                # if not, this is not desired, return an error message.
                else
                  echo "Current IP of nfs-server and the IP of nfs-share does not match. This is an undesired state."
                  echo "To fix, delete everything related to nfs-server and nfs-share, and run helm upgrade"
                  exit 1
                fi

              else    # nfs-share does not exist, create it
                echo "nfs-share does not exit, creating it..."

                cat <<EOF | kubectl apply -f -
              apiVersion: v1
              kind: PersistentVolume
              metadata:
                name: {{ include "peaka.fullname" . }}-nfs-share
                {{- if  .Values.nfsShare.volumeDeletionProtection }}
                annotations:
                  helm.sh/resource-policy: "keep"
                {{- end }}
              spec:
                capacity:
                  storage: {{ .Values.nfsShare.size }}
                accessModes:
                  - ReadWriteMany
                nfs:
                  path: {{ .Values.nfsShare.path | quote }}
                  server: $CLUSTER_IP
                persistentVolumeReclaimPolicy: {{ .Values.nfsShare.persistentVolumeReclaimPolicy }}

              ---

              apiVersion: v1
              kind: PersistentVolumeClaim
              metadata:
                name: {{ include "peaka.fullname" . }}-nfs-share
                {{- if  .Values.nfsShare.volumeDeletionProtection }}
                annotations:
                  helm.sh/resource-policy: "keep"
                {{- end }}
              spec:
                accessModes:
                  - ReadWriteMany
                volumeName: {{ include "peaka.fullname" . }}-nfs-share
                storageClassName: ""
                resources:
                  requests:
                    storage: {{ .Values.nfsShare.size }}
              EOF
              fi

      restartPolicy: OnFailure
